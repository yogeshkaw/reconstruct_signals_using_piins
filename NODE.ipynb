{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "\n",
    "from data_preprocessing import DataPreprocessing\n",
    "from create_torch_datset import CustomDataset\n",
    "from utils import get_batch, get_device\n",
    "\n",
    "sys.path.append(\"/Users/yogeshkawadkar/Documents/sysid-neural-continuous/torchid\")\n",
    "\n",
    "from ssmodels_ct import NeuralStateSpaceModel, DeepNeuralStateSpaceModel\n",
    "from ss_simulator_ct import ForwardEulerSimulator, ExplicitRKSimulator\n",
    "\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "device = 'cpu'\n",
    "\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_time_in_seconds = 0.5\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Overall parameters\n",
    "num_iter = 250 # 4*500-100  # gradient-based optimization steps\n",
    "seq_len = 128\n",
    "batch_size = 256 # batch size q\n",
    "lr = 1e-3  # learning rate\n",
    "test_freq = 1  # print message every test_freq iterations\n",
    "test_freq_val = 10 * test_freq # do validation every test_freq_val iterations\n",
    "# add_noise = True\n",
    "\n",
    "\n",
    "\n",
    "path_dataset = \"/Users/yogeshkawadkar/Desktop/data_augmentation_for_multivariate_time_series/datasets/Audi a2d2/camera_lidar/20180810_150607/bus/20180810150607_bus_signals.json\"\n",
    "input_signals = [\n",
    "    'accelerator_pedal',\n",
    "    'brake_pressure',\n",
    "    'steering_angle_calculated',\n",
    "    'steering_angle_calculated_sign',\n",
    "    # 'pitch_angle',\n",
    "    # 'roll_angle'\n",
    "]\n",
    "\n",
    "output_signals = [\n",
    "    'acceleration_x',\n",
    "    'acceleration_y',\n",
    "    'acceleration_z',\n",
    "    'angular_velocity_omega_x',\n",
    "    'angular_velocity_omega_y',\n",
    "    'angular_velocity_omega_z',\n",
    "    'vehicle_speed',\n",
    "    # 'latitude_degree',\n",
    "    # 'longitude_degree',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP = DataPreprocessing(path_dataset, input_signals, output_signals, sequence_length = 128)\n",
    "DP.sampling_with_interpolation()\n",
    "DP.normalize_train()\n",
    "\n",
    "DP.train_test_split()\n",
    "\n",
    "t_interp_train, U_train, X_train, t_interp_val, U_val,  X_val = DP.get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2629,), (2629,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_interp_train.shape ,  t_interp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2629, 4), (2629, 7))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "class LatentODEFunc(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LatentODEFunc, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LatentODE(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LatentODE, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.func = LatentODEFunc(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.linspace(0, 1, x.size(1))\n",
    "        return odeint(self.func, x, t, method='dopri5')\n",
    "\n",
    "class LatentODEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LatentODEModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.latent_ode = LatentODE(hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('input shape: {}'.format(x.shape))\n",
    "        encoded = self.encoder(x)\n",
    "        #print('encoded_dim : {}'.format(encoded.shape))\n",
    "        latent = self.latent_ode(encoded)\n",
    "        #print('latent_dim : {}'.format(latent.shape))\n",
    "        decoded = self.decoder(latent)\n",
    "        #print('decoded_dim : {}'.format(decoded.shape))\n",
    "        return decoded\n",
    "\n",
    "# Generate sample data\n",
    "# Assume you have a dataset of shape (num_samples, sequence_length, num_features)\n",
    "num_samples = 1000\n",
    "sequence_length = 128\n",
    "num_features = 4\n",
    "#input_data = torch.randn(num_samples, sequence_length, num_features)\n",
    "\n",
    "# Construct dependent signals\n",
    "#dependent_signals = torch.randn(num_samples, sequence_length, 7)\n",
    "\n",
    "# Concatenate independent and dependent signals\n",
    "#input_data = torch.cat((input_data, dependent_signals), dim=2)\n",
    "\n",
    "# Split into training and validation sets\n",
    "#train_size = int(0.8 * num_samples)\n",
    "#train_data = input_data[:train_size]\n",
    "#val_data = input_data[train_size:]\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 256\n",
    "#train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(TensorDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, u , x = get_batch(X_train, U_train, t_interp_train, batch_size, seq_len, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_val = torch.tensor(U_val.astype('float32')).to(device)\n",
    "X_val = torch.tensor(X_val.astype('float32')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n",
      "checkpoint2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(err_fit\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py:126\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.backward\u001b[0;34m(ctx, *grad_y)\u001b[0m\n\u001b[1;32m    123\u001b[0m     time_vjps[i] \u001b[38;5;241m=\u001b[39m dLd_cur_t\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Run the augmented system backwards in time.\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m aug_state \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugmented_dynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maug_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_rtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_atol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madjoint_options\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m aug_state \u001b[38;5;241m=\u001b[39m [a[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m aug_state]  \u001b[38;5;66;03m# extract just the t[i - 1] value\u001b[39;00m\n\u001b[1;32m    132\u001b[0m aug_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m y[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# update to use our forward-pass estimate of the state\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py:30\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_integrate(t)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[0;32m---> 30\u001b[0m     solution[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_advance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solution\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py:194\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_t \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt1:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m n_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_steps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_num_steps exceeded (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_steps)\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adaptive_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrk_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _interp_evaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39minterp_coeff, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt1, next_t)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py:255\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    250\u001b[0m         dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Must be arranged as doing all the step_t handling, then all the jump_t handling, in case we\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# trigger both. (i.e. interleaving them would be wrong.)\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m y1, f1, y1_error, k \u001b[38;5;241m=\u001b[39m \u001b[43m_runge_kutta_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtableau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtableau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# dtypes:\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# y1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# f1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#                     Error Ratio                      #\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m########################################################\u001b[39;00m\n\u001b[1;32m    265\u001b[0m error_ratio \u001b[38;5;241m=\u001b[39m _compute_error_ratio(y1_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol, y0, y1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/rk_common.py:76\u001b[0m, in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m         perturb \u001b[38;5;241m=\u001b[39m Perturb\u001b[38;5;241m.\u001b[39mNONE\n\u001b[1;32m     75\u001b[0m     yi \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(k[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (beta_i \u001b[38;5;241m*\u001b[39m dt), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview_as(f0)\n\u001b[0;32m---> 76\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     k \u001b[38;5;241m=\u001b[39m _UncheckedAssign\u001b[38;5;241m.\u001b[39mapply(k, f, (\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tableau\u001b[38;5;241m.\u001b[39mc_sol[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (tableau\u001b[38;5;241m.\u001b[39mc_sol[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tableau\u001b[38;5;241m.\u001b[39mbeta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mall()):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# This property (true for Dormand-Prince) lets us save a few FLOPs.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:159\u001b[0m, in \u001b[0;36m_ReverseFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:138\u001b[0m, in \u001b[0;36m_TupleFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 138\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_flat_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([f_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/adjoint.py:88\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.backward.<locals>.augmented_dynamics\u001b[0;34m(t, y_aug)\u001b[0m\n\u001b[1;32m     83\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# If using an adaptive solver we don't want to waste time resolving dL/dt unless we need it (which\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# doesn't necessarily even exist if there is piecewise structure in time), so turning off gradients\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# wrt t here means we won't compute that if we don't need it.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m func_eval \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt_requires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Workaround for PyTorch bug #39784\u001b[39;00m\n\u001b[1;32m     91\u001b[0m _t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_strided(t, (), ())  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 18\u001b[0m, in \u001b[0;36mLatentODEFunc.forward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, x):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlp/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "input_dim = num_features \n",
    "hidden_dim = 64\n",
    "out_dim = 7\n",
    "model = LatentODEModel(input_dim, hidden_dim, out_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "LOSS = []\n",
    "VAL_LOSS = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    t, u , x = get_batch(X_train, U_train, t_interp_train, batch_size, seq_len, device)\n",
    "    print('checkpoint')\n",
    "    #u = torch.tensor(U_train.astype('float32'))\n",
    "    #x = torch.tensor(X_train.astype('float32'))\n",
    "    optimizer.zero_grad()\n",
    "    output = model(u)[0,:,:]\n",
    "    err_fit = output - x\n",
    "    loss = torch.mean(err_fit**2)\n",
    "    print('checkpoint2')\n",
    "    loss.backward()\n",
    "    print('checkpoint3')\n",
    "    optimizer.step()\n",
    "    print('checkpoint4')\n",
    "\n",
    "    LOSS.append(loss.item())\n",
    "    \n",
    "    #break\n",
    "        #train_loss += loss.item() * data.size(0)\n",
    "    #train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      # u = torch.tensor(U_train.astype('float32'))\n",
    "       output = model(U_val)[0,:,:]\n",
    "\n",
    "       err_fit = output - X_val\n",
    "       val_loss = torch.mean(err_fit**2)\n",
    "       VAL_LOSS.append(val_loss.item())\n",
    "       \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}')\n",
    "\n",
    "# After training, you can use the model for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(LOSS, label='loss', color='blue')\n",
    "# plt.yscale('log')\n",
    "# plt.figure(figsize=(15,3))\n",
    "# plt.plot(VAL_LOSS, label='loss', color='orange')\n",
    "plt.plot( VAL_LOSS, label='loss', color='orange')\n",
    "plt.yscale('log')\n",
    "\n",
    "# plt.plot(LOSS_CONSISTENCY, label='consistency loss')\n",
    "# plt.plot(LOSS_FIT, label='fit loss')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u):\n",
    "    #x0 = x[0].astype('float32')\n",
    "    #x0_torch = torch.from_numpy(x0).to(device)\n",
    "    #u_torch = torch.tensor(u.astype('float32')).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_sim_torch = output = model(u)[0,:,:]\n",
    "        #x_sim = x_sim_torch.squeeze(1).cpu().numpy()\n",
    "\n",
    "    return x_sim_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2629, 7]), torch.Size([2629, 4]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape , u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.ode_func = ODEFunc(input_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x0, t):\n",
    "        out = odeint(self.ode_func, x0, t)\n",
    "        #print(out.shape)\n",
    "\n",
    "        #print(out)\n",
    "        #print('---')\n",
    "        return self.output_layer(out).shape\n",
    "\n",
    "# Example usage\n",
    "input_dim = 4\n",
    "hidden_dim = 64\n",
    "output_dim = 7\n",
    "seq_len = 10\n",
    "\n",
    "model = NeuralODE(input_dim*seq_len, hidden_dim, output_dim)\n",
    "\n",
    "# Sample input\n",
    "x0 = torch.randn(5,seq_len * input_dim)\n",
    "t = torch.linspace(0, 1, seq_len)\n",
    "\n",
    "output = model(x0, t)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5723,  0.5308,  0.7280,  0.2005,  0.3682, -1.0152,  0.0409,  0.0976],\n",
       "        [-0.3128, -0.7258,  0.6599, -0.3286, -1.1099, -0.3897,  0.7563,  0.2068],\n",
       "        [ 0.4298, -0.9723,  0.5377,  0.1554,  0.3651,  0.2270, -0.2189, -0.6674],\n",
       "        [-0.6834,  0.7757, -0.3257, -0.5602,  0.0563,  0.7473,  1.2284, -1.1450],\n",
       "        [ 0.7359, -0.1982, -0.3115,  0.9176, -1.0079, -0.0463, -0.0331,  0.6049]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch_size, seq_len, num_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
